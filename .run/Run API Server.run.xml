<component name="ProjectRunConfigurationManager">
  <configuration default="false" name="Run API Server" type="PythonConfigurationType" factoryName="Python">
    <module name="ChatGLM3" />
    <option name="ENV_FILES" value="$PROJECT_DIR$/openai_api_demo/.env" />
    <option name="INTERPRETER_OPTIONS" value="" />
    <option name="PARENT_ENVS" value="true" />
    <envs>
      <env name="LOCAL_EMBEDDING_MODEL_PATH" value="$PROJECT_DIR$/../bge-large-zh-v1.5" />
      <env name="LOCAL_MODEL_PATH" value="$PROJECT_DIR$/../chatglm3-6b" />
    </envs>
    <option name="SDK_HOME" value="" />
    <option name="SDK_NAME" value="py3.10" />
    <option name="WORKING_DIRECTORY" value="" />
    <option name="IS_MODULE_SDK" value="false" />
    <option name="ADD_CONTENT_ROOTS" value="true" />
    <option name="ADD_SOURCE_ROOTS" value="true" />
    <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
    <option name="SCRIPT_NAME" value="$PROJECT_DIR$/openai_api_demo/api_server.py" />
    <option name="PARAMETERS" value="" />
    <option name="SHOW_COMMAND_LINE" value="false" />
    <option name="EMULATE_TERMINAL" value="false" />
    <option name="MODULE_MODE" value="false" />
    <option name="REDIRECT_INPUT" value="false" />
    <option name="INPUT_FILE" value="" />
    <method v="2" />
  </configuration>
</component>